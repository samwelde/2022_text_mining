{
  "metadata": {
    "kernelspec": {
      "name": "python",
      "display_name": "Pyolite",
      "language": "python"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    }
  },
  "nbformat_minor": 4,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": "# 2022_03_31",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "**Author:**\n[Samwelde](https://github.com/samwelde)",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "# **Text mining example** Kiva.org website for loans\"",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "## Dependencies",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "## Libraries\nlibrary(\"tm\")\nlibrary(\"data.table\")\nlibrary(\"ggplot2\")\nlibrary(\"tidytext\")\nlibrary(\"dplyr\")\nlibrary(\"topicmodels\")\nlibrary(\"wordcloud\")\nlibrary(\"SentimentAnalysis\")\nlibrary(\"naivebayes\")\nlibrary(\"slam\")\nlibrary(\"glmnet\")\nlibrary(\"lexicon\")\nlibrary(\"fastNaiveBayes\")\nlibrary(\"caret\")\nlibrary(\"ranger\")",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "## Simple helper function to view first copora elements, only for illustration in lecture\nchead <- function(c) lapply(c[1:2], as.character)",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "## Setting up a corpus and applying transformations",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "This tutorial relies on the Kiva data for crowdfunded loans. The following analyses are based on the loan description and to a limited extent the loan purpose statement as well. The data is based on a csv database dump provided on their site. To ease computation we just use a limited sample of 10,000 observations. Using the csv file from the Kiva Homepage, you can clean the data yourself and use a larger sample. The full sample is close to one million observations. ",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "I spent some time pre-processing the data, filtering HTML tags and similar things, to get mostly clean loan descriptions. This type of pre-processing is common, but also very application specific. If you prep the data yourself, you may need to delete a few nested parentheses in the loan description using a text editor to make it readable. ",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "In case you are using Windows, or any other OS not using UTF-8 encoding as default, setting the encoding when reading data files is good practice. When working with text data, take care to ensure you are using the correct encoding and that transformations between files and encodings do not lead to broken characters.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "## Read data\nloans <- data.table::fread(\"Data/kiva-tiny.csv\", encoding = \"UTF-8\")\nnames(loans)",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "The first thing we are going to do is set up a corpus. We will focus on the loan description. For all transformations in the remainder of this tutorial, we are going to print the first two loans' descriptions for illustration.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "## Set up corpus\nsetnames(loans, \"loanid\", \"doc_id\")\nsetnames(loans, \"description\", \"text\")\ncorp <- Corpus(DataframeSource(loans))\n\n## Inspect it\ncorp\nlapply(corp[1:2], as.character)",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "These are the main transformations available in the `tm` library, but any other customized transformation can be applied.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "## Main corpus transformations, passed via tm_map()\n## Other transformations have to be wrapped in content_transformer()\ngetTransformations()",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "tolower(\"allcAPS\")",
      "metadata": {
        "scrolled": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "We apply the `base` string function `tolower()` to transform all strings to lower case.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "## All chars to lower case\ncorp <- tm_map(corp, content_transformer(tolower))\nchead(corp)",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "Remove all punctuation as punctuation is unlikely to carry special meaning in the context of loans and we want to simplify the text input to get token counts. We need to set the unicode option to true to rid of all punctuation elements (eg. the quotation marks).",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "## Remove punctuation\n# corp <- tm_map(corp, removePunctuation)\n# chead(corp)\ncorp <- tm_map(corp, removePunctuation, ucp = TRUE)\nchead(corp)",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "Now we remove all numbers. We observe the loan amount and the repayment schedule in other variables, so we can get rid of numbers. Extracting the meaning of numbers within their context is difficult. ",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "## Remove numbers\ncorp <- tm_map(corp, removeNumbers)\nchead(corp)",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "Any other transformation - like substituting specific patterns based on regular expressions - can be passed to `tm_map()` using a user-defined function.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "## For specific transformations, you could also pass a lambda function to remove patterns based on a regex\n\n## Example:\n# toSpace <- content_transformer(function (x , pattern) gsub(pattern, \" \", x))\n# corp <- tm_map(corp, toSpace, \"ocongate\")\n# chead(corp)",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "Looking at a frequency plot of the token counts, there is still plenty of filtering to do to get meaningful token counts.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "## Look at the most frequent words in our text and see whether we should get rid of some\n# frequent_terms <- qdap::freq_terms(corp, 30)\n# plot(frequent_terms)",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "Next, we remove stopwords and other generic words which do not carry special meaning in our context.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "## More invasive changes: remove generic and custom stopwords\ncorp <- tm_map(corp, removeWords, stopwords('english'))\nchead(corp)",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "## And a few more words we filter for lack of being informative, this could be extended\ncorp <- tm_map(corp, removeWords, \"loan\")\ncorp <- tm_map(corp, removeWords, \"kiva\")",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "## There are a lot of names in the data, these are not really informative\n## We apply a dictionary to get rid of some of them\n## Truncation because of regex limit\ncorp <- tm_map(corp, removeWords, common_names[1:floor(length(common_names)/2)])\ncorp <- tm_map(corp, removeWords, common_names[floor(length(common_names)/2):length(common_names)])\ncorp <- tm_map(corp, removeWords, freq_first_names[1:floor(nrow(freq_first_names)/2), Name])\ncorp <- tm_map(corp, removeWords, freq_first_names[floor(nrow(freq_first_names)/2):nrow(freq_first_names), Name])\n## corp <- tm_map(corp, removeWords, freq_last_names) # needs to be truncated as well, even longer\nchead(corp)",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "common_names[1:100]",
      "metadata": {
        "scrolled": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "You can also stem the document here. For illustration purposes I refrain from it here, but in real applications you might want to do this. Stemmers do not work equally well for all languages, depending on your application, there may be added value to grouping and transforming tokens further yourself.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "## Stem document\n## corp <- tm_map(corp, stemDocument, language = 'english')\n## chead(corp)",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "Strip all extra whitespace (this is without consequences for tokenization).",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "## Strip extra whitespace\ncorp <- tm_map(corp, stripWhitespace) \nchead(corp)",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "## Building a document-term matrix and restricting the feature set",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "We transform the corpus to a document-term matrix. We use simple term-frequency weighting, i.e. the simple token counts. This is the default. You can also choose term frequency-inverse document frequency (tfidf) weighting at this point.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "## Build a document-term or term-document matrix\n## Default is term-frequency weighting (document length normalized count)\n## TF-IDF weighting also possible\n\n## dtm <- TermDocumentMatrix(corp)\ndtm <- DocumentTermMatrix(corp)\n\n## Inspect the document-term matrix\ninspect(dtm)",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "230763649+516351",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "Following the tokenization, we can inspect the most popular words.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "## Inspect most popular words\nfindFreqTerms(dtm, lowfreq = 1000)",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "The document-word vectors allow us to inspect the correlation between words as we would between variables in other data.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "## Inspect associations\nfindAssocs(dtm, 'profit', 0.15)",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "Since the matrix is very wide and sparse, we are going to remove terms to arrive at a tractable representation. We can filter words simply by removing words that are very rare.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "## Remove sparse terms, prevents cluster node from choking and saves time\n## Don't do this at all if you can avoid it (or limit only as little as possible).\n\n## Tweak the sparse parameter to influence # of words\ndtms <- removeSparseTerms(dtm, sparse = 0.90)\n# dtms <- removeSparseTerms(dtm, sparse = 0.95) ## less sparse, works much better for modeling, computation longer\ndim(dtms)\ndtms <- dtms[row_sums(dtms) > 0, ]\ndim(dtms)",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "We can also filter by tf-idf, only keeping words which occur frequently in some documents but not in others, helping us to keep those words that disambiguate loans. We compute the average tf-idf score for each token, then filter by that. I went back-and-forth a bit tweaking the threshold value for filtering.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "## Alternatively, filter words by mean tf-idf\n\n## Calculate average term-specific tf-idf weights as\n## mean(word count/document length) * log(ndocs/ndocs containing word)\ntermtfidf <- tapply(dtm$v/row_sums(dtm)[dtm$i], dtm$j, mean) *\n             log(nDocs(dtm)/col_sums(dtm > 0))\nsummary(termtfidf)",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "## Only include terms with at least median tf-idf score\ndtmw <- dtm[, (termtfidf >= 0.15)]\ndim(dtmw)\n## And documents within which these terms occur - this may induce selection\ndtmw <- dtmw[row_sums(dtmw) > 0, ]\ndim(dtmw)\ndim(dtm)",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "## Much less frequent terms now\nfindFreqTerms(dtmw, lowfreq=100)",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "## Visualizations of word frequencies",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "These are just very simple visualizations of word frequencies. First the unfiltered but transformed corpus.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "## Simple visualization\nwordcloud(corp, max.words = 100, random.order = FALSE,\n          colors = brewer.pal(8, \"Dark2\"))",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "Next the term-frequency filtered document-term matrix. Obviously this is very similar to the plot above. ",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "## Counts from dtms\ncounts <- sort(colSums(as.matrix(dtms)), decreasing = TRUE)\ncounts <- data.frame(word = names(counts), freq = counts)\nwordcloud(words = counts$word, freq = counts$freq,\n          max.words = 100, random.order = FALSE,\n          colors = brewer.pal(8, \"Dark2\"))",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "However, the frequency plot of the tfidf-filtered document-term matrix looks different. There are a lot more terms that disambiguate professions and investment goods.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "## Counts from dtmw\ncounts <- sort(colSums(as.matrix(dtmw)), decreasing = TRUE)\ncounts <- data.frame(word = names(counts), freq = counts)\nwordcloud(words = counts$word, freq = counts$freq,\n          max.words = 100, random.order = FALSE,\n          colors = brewer.pal(8, \"Dark2\"))",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "## Dictionary methods: Inferring sentiment",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "We are using a fixed mapping of terms to infer a sentiment score, and then convert it to discrete sentiment categories. Unsuprisingly, most loan descriptions are phrased to convey a positive message. ",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "## Dictionary method: Sentiment analysis using dictionaries\nsentiment <- analyzeSentiment(dtms, language = \"english\")\nsentiment <- convertToDirection(sentiment$SentimentGI)\n\n## look at sentiment distribution\ntable(sentiment)",
      "metadata": {
        "scrolled": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "## Unsupervised generative model: Topic model",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "Next we are going to train an unsupervised topic model on the term-frequency filtered document term matrix. You will find that it is hard to find distinct topics, both due to the term filtering, and the fact that most loans are handed out by partner organizations who use a standard questionnaire to get basic information which is then translated to an english description. ",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "## Unsupervised method: Topic model\n## lda <- LDA(dtms, k = 5, control = list(seed = 100))\nlda <- LDA(dtmw, k = 5, control = list(seed = 1000))\n\n## Most likely topic for each document, could merge this to original data\n## topic <- topics(lda, 1)\n\n## Five most frequent terms for each topic\nterms(lda, 10)\n\n## Plot most frequent terms and associated probabilities by topic\ntpm <- tidy(lda, matrix = \"beta\")\n\ntopterms <-\n    tpm %>%\n    group_by(topic) %>%\n    top_n(10, beta) %>%\n    ungroup() %>%\n    arrange(topic, -beta)\n\ntopterms %>%\n    mutate(term = reorder(term, beta)) %>%\n    ggplot(aes(term, beta, fill = factor(topic))) +\n    geom_col(show.legend = FALSE) +\n    facet_wrap(~ topic, scales = \"free\") +\n    coord_flip()",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "terms(lda, 10)",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "Instead, let us use the `loan use` statement, filtered by tfidf. These topics already look more distinct. You could tweak the filter and the number of topics further to arrive at a more meaningful result.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "## not working well due to standardized templates\n## let us try to use the `loanuse' statement text for the generative topic model \n\n# new data\nloanuse <- loans[, .(doc_id, loanuse)]\nsetnames(loanuse, \"loanuse\", \"text\")\n\n# new dtm, this time do most of the transformations in one step\ndtmuse <- DocumentTermMatrix(Corpus(DataframeSource(loanuse)),\n                             control = list(weighting = weightTf,\n                                            language = \"english\",\n                                            tolower = TRUE,\n                                            removePunctuation = TRUE,\n                                            removeNumbers = TRUE,\n                                            stopwords = TRUE,\n                                            stemming = FALSE,\n                                            wordLengths = c(3, Inf)))\ninspect(dtmuse)\n",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "str(dtmuse)",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "# Recalculate weights\ntermtfidf <- tapply(dtmuse$v/row_sums(dtmuse)[dtmuse$i], dtmuse$j, mean) *\n    log2(nDocs(dtmuse)/col_sums(dtmuse > 0))\nsummary(termtfidf)\n\n## Filter by tf-idf\n## dim(dtmuse)\ndtmuse.tfidf <- dtmuse[, (termtfidf >= 1.30)]\ndtmuse.tfidf <- dtmuse.tfidf[row_sums(dtmuse.tfidf) > 0, ]\ndim(dtmuse.tfidf)",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "## Unsupervised method: Topic model, this time for loanuse statement\nlda <- LDA(dtmuse.tfidf, k = 5, control = list(seed = 1000))\n## str(lda)",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "## Unsupervised method: Topic model, this time for loanuse statement\nlda <- LDA(dtmuse, k = 5, control = list(seed = 1000))\n## str(lda)",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "## Most likely topic for each document, could merge this to original data\ntopic <- topics(lda, 1)\nhead(topic)",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "## Five most frequent terms for each topic\nterms(lda, 10)",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "## Plot most frequent terms and associated probabilities by topic\ntpm <- tidy(lda, matrix = \"beta\")\n\ntopterms <-\n    tpm %>%\n    group_by(topic) %>%\n    top_n(10, beta) %>%\n    ungroup() %>%\n    arrange(topic, -beta)\n\ntopterms %>%\n    mutate(term = reorder(term, beta)) %>%\n    ggplot(aes(term, beta, fill = factor(topic))) +\n    geom_col(show.legend = FALSE) +\n    facet_wrap(~ topic, scales = \"free\") +\n    coord_flip()",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "Look at unique terms not appearing in other topics.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "freqterms <- terms(lda, 40)\nduplicates <- c(freqterms)[duplicated(c(freqterms))]\ndistinctterms <- lapply(as.list(as.data.frame(freqterms)), function(x) x[!(x %in% duplicates)])\ndistinctterms",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "## Supervised methods: Data preparation",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "The following cells transform and prep the data to be used as inputs for supervised methods. We split the data into a test and a training sample. The outcome we try to predict is whether the loan is obtained for a business proposition in the agricultural sector.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "## Input: Only filtering here for tractability and runtime. \n## Use dtm without restrictions or leave sparsity as large as possible.\ndtms <- removeSparseTerms(dtm, sparse = 0.95)\ndim(dtms)",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "## Supervised methods: Prep data\n## Convert the sparse term-document matrix to a standard data frame\nbag <- as.data.frame(as.matrix(dtms))\ndim(bag)\n#bag\nhead(bag)",
      "metadata": {
        "scrolled": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "## Convert token counts to simple binary indicators\nbag.bin <- as.data.frame(sapply(bag, function(x) as.numeric(x > 0)))\ndim(bag.bin)\nhead(bag.bin)\nhist(bag$buy)",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "## Add names to rows\nbag$doc_id <- rownames(as.matrix(dtms))\nbag.bin$doc_id <- rownames(as.matrix(dtms))\nhead(bag)",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "## Different sectors\ntable(loans$sectorname)",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "## Add outcomes from the original data: Predict agricultural sector\nloans$agsector <- as.numeric(loans$sectorname == \"Agriculture\")\nbag <- merge(bag, loans[, .(agsector, loanamount, doc_id)], by = \"doc_id\")\nbag.bin <- merge(bag.bin, loans[, .(agsector, loanamount, doc_id)], by = \"doc_id\")\n                            \n# How many people want a loan in the agricultural sector?                            \ntable(bag$agsector)",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "## Partition data in test and training sample\nset.seed(100)\ntestids <- sample(floor(nrow(bag)/5))",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "names(bag)",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "xtrain <- as.matrix(bag[-testids, !(names(bag) %in% c(\"agsector\", \"loanamount\", \"doc_id\"))])\nxtest  <- as.matrix(bag[ testids, !(names(bag) %in% c(\"agsector\", \"loanamount\", \"doc_id\"))])\n\nxtrain.bin <- as.matrix(bag.bin[-testids, !(names(bag) %in% c(\"agsector\", \"loanamount\", \"doc_id\"))])\nxtest.bin  <- as.matrix(bag.bin[ testids, !(names(bag) %in% c(\"agsector\", \"loanamount\", \"doc_id\"))])\n\nytrain <- as.factor(bag[-testids,  \"agsector\"])\nytest  <- as.factor(bag[ testids,  \"agsector\"])\n\ndim(xtrain)\nlength(ytrain)\n\ndim(xtest)\nlength(ytest)",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "## Supervised generative model: Naive Bayes classifier",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "### With binary token indicators",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "Naive Bayes is a simple model relying on a conditional independence assumption of the token counts. It often performs acceptable. In this case it does not perform very well, possibly because we filtered the input token data to aggressively. If you redo the analysis with a larger set of features (setting sparsity = 0.95, or not filtering at all), the naive bayes classifier performs much better. Among other things, there is also no need to remove stopwords here, and it may not improve performance. You can feed the unfiltered data or try different transformations and see whether this improves matters. ",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "## Supervised generative model: Naive Bayes\n## naive_bayes package requires transforming everything to factors and using binary indicators, not counts.\nxtrain.factor <- as.data.frame(lapply(as.data.frame(xtrain.bin), as.factor))\nxtest.factor <- as.data.frame(lapply(as.data.frame(xtest.bin), as.factor))",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "nbclassifier <- naive_bayes(xtrain.factor, ytrain, laplace = 1)\nnbclassifier",
      "metadata": {
        "scrolled": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "nbpred <- predict(nbclassifier, xtest.factor)\n# nbclassifier\nsummary(nbpred)",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "## Performance statistics: Classification rate\nround(1 - mean(as.numeric(nbpred != ytest)), 2)",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "## Performance statistics: Confusion matrix (\n## table(nbpred, ytest)\nconfusionMatrix(nbpred, ytest)",
      "metadata": {
        "scrolled": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "expand.grid(\n    mtry = seq(2, 2 * floor(sqrt(ncol(xtrain))), length.out = 10),\n    splitrule = \"gini\",\n    min.node.size = c(1,3)\n  )",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "#### Tuning the laplace smoothing parameter\n\nThere isn't really much scope for tuning with naive bayes.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "## parameter grid\nnb.grid <- expand.grid(\n  laplace = seq(0, 1, 0.1),\n  adjust = 1,\n  usekernel = TRUE\n)\nnb.grid",
      "metadata": {
        "scrolled": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "## use k-fold cv to tune the laplace smoothing parameter\nnbclassifier <- train(\n  xtrain.factor, ytrain,\n  method = \"naive_bayes\",\n  trControl = trainControl(method = \"cv\", number = 10),\n  tuneGrid = nb.grid\n)\n\nnbclassifier\nsummary(nbclassifier)",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "nbpred <- predict(nbclassifier, xtest.factor)\n1-mean(as.numeric(nbpred != ytest))\nconfusionMatrix(nbpred, ytest)",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "### With token counts",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "Binary features perform only slightly better/worse, mostly just about the same depending on then size of the design matrix. Whether a word occurs at all encodes about the same information compared to how frequent it occurs. ",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "## fastNaiveBayes is the better package (supports multinomial distribution, for non-binary feature counts)\n## fnb.detect_distribution(xtrain)\n\nnbclassifier <- fastNaiveBayes(xtrain, ytrain)\n# nbclassifier <- multinomial_naive_bayes(xtrain, ytrain)\n\nnbpred <- predict(nbclassifier, xtest)",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "## Performance statistics: Classification rate\nround(1-mean(as.numeric(nbpred != ytest)), 2)\n\n## Performance statistics: Confusion matrix (\n## table(nbpred, ytest)\nconfusionMatrix(nbpred, ytest)",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "## Supervised text regression: L<sub>1</sub> penalized logistic classifier",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "Looking at the misclassification rate and the confusion matrix, the model performs better than naive bayes in predicting the agricultural sector. However, looking at precision and recall, the model again does poorly in getting the true condition outcomes right, leading to a large number of false negatives.",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "### With binary token indicators",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "## Supervised text regression: L1 penalized logistic regression\nl1classifier <- cv.glmnet(xtrain.bin, ytrain, alpha = 1, family = \"binomial\")\nl1pred <- as.factor(predict(l1classifier, xtest.bin, s = \"lambda.min\", type = \"class\"))\nsummary(l1pred)",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "plot(l1classifier)",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "## Performance statistics: Classification rate\nround(1-mean(as.numeric(l1pred != ytest)), 2)\n\n## Performance statistics: Confusion matrix\ncaret::confusionMatrix(l1pred, ytest)",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "### With token counts",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "If you check, the model with feature counts does better than the binary model, but only marginally.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "## Supervised text regression: L1 penalized logistic regression\nl1classifier <- cv.glmnet(xtrain, ytrain, alpha = 1, family = \"binomial\")\nl1pred <- as.factor(predict(l1classifier, xtest, s = \"lambda.min\", type = \"class\"))\nsummary(l1pred)",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "plot(l1classifier)",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "## Performance statistics: Classification rate\nround(1-mean(as.numeric(l1pred != ytest)), 2)\n\n## Performance statistics: Confusion matrix\ncaret::confusionMatrix(l1pred, ytest)",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "This also trains a logistic lasso estimator, weighting the penalty factor for each input token by the token's standard deviation. Results do not really differ compared to just standardizing (no surprise).",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "## L1 logistic classifier using rare feature upweighting\n# l1classifier <- cv.glmnet(xtrain, ytrain, alpha = 1, family = \"binomial\")\n## L1 logistic classifier using rare feature upweighting\nsdweights <- apply(xtrain, 2, sd)\nl1classifier <- cv.glmnet(xtrain, ytrain, alpha = 1, family = \"binomial\",\n                          standardize = FALSE, penalty.factor  = sdweights)\nl1pred <- as.factor(predict(l1classifier, xtest, s = \"lambda.min\", type = \"class\",\n                            penalty.factor  = sdweights))\nsummary(l1pred)",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "## Performance statistics: Classification rate\nround(1-mean(as.numeric(l1pred != ytest)), 2)\n## Performance statistics: Confusion matrix\ncaret::confusionMatrix(l1pred, ytest)",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "## Supervised: Random forest",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "## using library(caret) for training. alternative: library(mlr), or library(tidymodels) if you prefer tidyverse \n\n## using oob\n## small scale for cluster, adjust mtry to finer grid and increase num.trees substantially\nrfclassifier <- train(\n  y = ytrain,\n  x = xtrain,\n  method = \"ranger\",\n  num.trees = 200,\n  tuneGrid = expand.grid(\n    mtry = seq(2, 2 * floor(sqrt(ncol(xtrain))), length.out = 10),\n    splitrule = \"gini\",\n    min.node.size = c(1,3)\n  ),\n  trControl = trainControl(\n    method = \"oob\"\n  )\n)",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "rfclassifier",
      "metadata": {
        "scrolled": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "plot(rfclassifier)",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "rfclassifier$bestTune",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "rfpred <- predict(rfclassifier, xtest)\n1 - mean(as.numeric(rfpred != ytest))",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "confusionMatrix(rfpred, ytest)",
      "metadata": {
        "scrolled": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "## Boosted trees",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "## Ada boost with decision tree as base learner\n## Simplified to lower runtime!\n## Increase iterations and use larger parameter grid, expand grid for maxdepth, user finer grid for learning rate.\ngbclassifier <- train(\n  y = ytrain,\n  x = xtrain,\n  method = \"ada\",\n  tuneGrid = expand.grid(\n    iter = 10, \n    maxdepth = seq(2, 5, 1), \n    nu = seq(0.1, 1, 0.3)\n  ),\n  trControl = trainControl(\n    method = \"cv\",\n    number = 5\n  )\n)",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "gbclassifier",
      "metadata": {
        "scrolled": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "plot(gbclassifier)",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "gbclassifier$bestTune",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "gbpred <- predict(gbclassifier, xtest)\n1 - mean(as.numeric(gbpred != ytest))",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "confusionMatrix(gbpred, ytest)",
      "metadata": {
        "scrolled": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "## gradient boosting\n## try method = \"xgbTree\" from library(xgboost), may have better performance but more tuning parameters\n## ...",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "# Tasks",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "## Task: The above examples are all classification. Implement a regression example. \n## Predict the loanamount using L1 penalized linear regression (lasso).\n## ...",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "## Discuss: How would you go about improving performce for the classifiers?\n## Do not restrict sparsity/drop columns for tractability. \n## Consider adding other predictors as inputs. \n## Use the loanuse statement.",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "## Task: Re-implement the classifiers using only the loan use statement. \n## ...",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "# Addendum",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "## Regression example: Predicting loanamount using L<sub>1</sub> penalized linear regression",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "## Further example: Predict Loan Amount\n## Supervised text regression: L1 penalized linear regression\n\n## Rebuild outcome vectors\nytrain <- as.matrix(bag[-testids,  \"loanamount\"])\nytest  <- as.matrix(bag[ testids,  \"loanamount\"])",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "## Estimate and predict\nl1predictor <- cv.glmnet(xtrain, ytrain, alpha = 1, family = \"gaussian\")\nl1pred <- predict(l1predictor, xtest, s = \"lambda.min\", type = \"response\")\n",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "## RMSE\nround(sqrt(mean((l1pred - ytest)^2)), 2)\npostResample(l1pred, ytest)",
      "metadata": {
        "scrolled": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "hist(ytrain)",
      "metadata": {
        "scrolled": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "## Understanding signal-to-noise",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "This section demonstrates the value of good data.",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": " \n## Using the 'loanuse' statement for prediction",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "## Input: Only filtering here for tractability and runtime. \n## Use dtm without restrictions or leave sparsity as large as possible.\ndtms <- removeSparseTerms(dtmuse, sparse = 0.995)\ndim(dtms)",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "## Supervised methods: Prep data\n## Convert the sparse term-document matrix to a standard data frame\nbag <- as.data.frame(as.matrix(dtms))\n## Convert token counts to simple binary indicators\nbag.bin <- as.data.frame(sapply(bag, function(x) as.numeric(x > 0)))",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "## Add names to rows\nbag$doc_id <- rownames(as.matrix(dtms))\nbag.bin$doc_id <- rownames(as.matrix(dtms))\nhead(bag)",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "## Add outcomes from the original data: Predict agricultural sector\nloans$agsector <- as.numeric(loans$sectorname == \"Agriculture\")\nbag <- merge(bag, loans[, .(agsector, loanamount, doc_id)], by = \"doc_id\")\nbag.bin <- merge(bag.bin, loans[, .(agsector, loanamount, doc_id)], by = \"doc_id\")\n                            \n# How many people want a loan in the agricultural sector?                            \ntable(bag$agsector)",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "## Partition data in test and training sample\nset.seed(100)\ntestids <- sample(floor(nrow(bag)/5))",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "xtrain <- as.matrix(bag[-testids, !(names(bag) %in% c(\"agsector\", \"loanamount\", \"doc_id\"))])\nxtest  <- as.matrix(bag[ testids, !(names(bag) %in% c(\"agsector\", \"loanamount\", \"doc_id\"))])\n\nxtrain.bin <- as.matrix(bag.bin[-testids, !(names(bag) %in% c(\"agsector\", \"loanamount\", \"doc_id\"))])\nxtest.bin  <- as.matrix(bag.bin[ testids, !(names(bag) %in% c(\"agsector\", \"loanamount\", \"doc_id\"))])\n\nytrain <- as.factor(bag[-testids,  \"agsector\"])\nytest  <- as.factor(bag[ testids,  \"agsector\"])\n\ndim(xtrain)\nlength(ytrain)\n\ndim(xtest)\nlength(ytest)",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "## Running Naive Bayes on only the loan use statement",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "## Supervised generative model: Naive Bayes\n## naive_bayes package requires transforming everything to factors and using binary indicators, not counts.\noptions(encoding = \"UTF-8\")\nxtrain.factor <- as.data.frame(lapply(as.data.frame(xtrain.bin), as.factor))\nxtest.factor <- as.data.frame(lapply(as.data.frame(xtest.bin), as.factor))",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "nbclassifier <- naive_bayes(xtrain.factor, ytrain, laplace = 1)",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "nbpred <- predict(nbclassifier, xtest.factor)\n# nbclassifier\nsummary(nbpred)",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "## Performance statistics: Classification rate\nround(1 - mean(as.numeric(nbpred != ytest)), 2)\n\n## Performance statistics: Confusion matrix (\n## table(nbpred, ytest)\nconfusionMatrix(nbpred, ytest)",
      "metadata": {
        "scrolled": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "## Running Lasso on only the loan use statement",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "## Supervised text regression: L1 penalized logistic regression\nl1classifier <- cv.glmnet(xtrain.bin, ytrain, alpha = 1, family = \"binomial\")\nl1pred <- as.factor(predict(l1classifier, xtest.bin, s = \"lambda.min\", type = \"class\"))\nsummary(l1pred)",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "confusionMatrix(l1pred, ytest)",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "## Re-estimating the models using both loan use statement and description as inputs\n\nThis actually worsens the signal to noise ratio.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "bag.use <- as.data.frame(as.matrix(removeSparseTerms(dtmuse, sparse = 0.995)))\nbag.use.bin <- as.data.frame(sapply(bag.use, function(x) as.numeric(x > 0)))\nbag.desc <- as.data.frame(as.matrix(removeSparseTerms(dtm, sparse = 0.95)))\nbag.desc.bin <- as.data.frame(sapply(bag.desc, function(x) as.numeric(x > 0)))\n\nset.seed(100)\ntestids <- sample(floor(nrow(bag.use)/5))\n\nxtrain.use <- as.matrix(bag.use[-testids, !(names(bag.use) %in% c(\"agsector\", \"loanamount\", \"doc_id\"))])\nxtest.use  <- as.matrix(bag.use[ testids, !(names(bag.use) %in% c(\"agsector\", \"loanamount\", \"doc_id\"))])\nxtrain.desc <- as.matrix(bag.desc[-testids, !(names(bag.desc) %in% c(\"agsector\", \"loanamount\", \"doc_id\"))])\nxtest.desc  <- as.matrix(bag.desc[ testids, !(names(bag.desc) %in% c(\"agsector\", \"loanamount\", \"doc_id\"))])\n\nxtrain.use.bin <- as.matrix(bag.use.bin[-testids, !(names(bag.use) %in% c(\"agsector\", \"loanamount\", \"doc_id\"))])\nxtest.use.bin  <- as.matrix(bag.use.bin[ testids, !(names(bag.use) %in% c(\"agsector\", \"loanamount\", \"doc_id\"))])\nxtrain.desc.bin <- as.matrix(bag.desc.bin[-testids, !(names(bag.desc) %in% c(\"agsector\", \"loanamount\", \"doc_id\"))])\nxtest.desc.bin  <- as.matrix(bag.desc.bin[ testids, !(names(bag.desc) %in% c(\"agsector\", \"loanamount\", \"doc_id\"))])\n\nxtrain <- cbind(xtrain.use, xtrain.desc)\nxtrain.bin <- cbind(xtrain.use.bin, xtrain.desc.bin)\nxtest <- cbind(xtest.use, xtest.desc)\nxtest.bin <- cbind(xtest.use.bin, xtest.desc.bin)\n                                     \nytrain <- as.factor(as.numeric(loans$sectorname == \"Agriculture\")[-testids])\nytest  <- as.factor(as.numeric(loans$sectorname == \"Agriculture\")[testids])\n\ndim(xtrain.use)\ndim(xtrain.desc)\ndim(xtrain)\ntable(ytrain)",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "## Naive Bayes\nxtrain.factor <- as.data.frame(lapply(as.data.frame(xtrain.bin), as.factor))\nxtest.factor <- as.data.frame(lapply(as.data.frame(xtest.bin), as.factor))\nnbclassifier <- naive_bayes(xtrain.factor, ytrain, laplace = 1)\nnbpred <- predict(nbclassifier, xtest.factor)\nsummary(nbpred)\nconfusionMatrix(nbpred, ytest)",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "## Supervised text regression: L1 penalized logistic regression\nl1classifier <- cv.glmnet(xtrain.bin, ytrain, alpha = 1, family = \"binomial\")\nl1pred <- as.factor(predict(l1classifier, xtest.bin, s = \"lambda.min\", type = \"class\"))\nsummary(l1pred)\nconfusionMatrix(l1pred, ytest)",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    }
  ]
}